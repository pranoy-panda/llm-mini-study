{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook/00_setup.pynb\n",
    "'''\n",
    "load tokenizer & models, load small subset of WikiText-2 and a classification dataset English subset.\n",
    "Run baseline eval of pretrained gpt2 on WikiText-2 (record perplexity) and simple prompt eval on SST-2 (zero-shot scoring).\n",
    "'''\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f63d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --------- Load model & tokenizer ----------\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a682c4d",
   "metadata": {},
   "source": [
    "![AutoModel + AutoTokenizer diagram](../media/automodalcausallm_tokenizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fea110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 usually has no pad token; set pad_token to eos_token for batching\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.resize_token_embeddings(len(tokenizer))  # safe no-op if unchanged\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def batchify_tokenized(batch):\n",
    "    # batch is a list of dictionaries, each with 'input_ids'\n",
    "    input_ids_list = [torch.tensor(x[\"input_ids\"], dtype=torch.long) for x in batch]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_list,\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id,\n",
    "    )\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "\n",
    "# Tokenize examples (no truncation; we will feed variable-length sequences)\n",
    "def tokenize_wiki(example):\n",
    "    enc = tokenizer(example[\"text\"], add_special_tokens=False)\n",
    "    return {\"input_ids\": enc[\"input_ids\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e259d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- WikiText-2 Loading ----------\n",
    "# Load small subset (validation). wikitext-2-raw-v1 keeps original whitespace.\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "\n",
    "# Subsample: choose a small number for quick runs on Colab; increase if needed.\n",
    "NUM_WIKI_EXAMPLES = 512\n",
    "wikitext = wikitext.select(range(min(len(wikitext), NUM_WIKI_EXAMPLES)))\n",
    "wikitext_tok = wikitext.map(tokenize_wiki, remove_columns=wikitext.column_names)\n",
    "\n",
    "# DataLoader\n",
    "BATCH_SIZE_WIKI = 8\n",
    "wiki_loader = DataLoader(wikitext_tok, batch_size=BATCH_SIZE_WIKI, collate_fn=batchify_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b05ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- WikiText-2 Perplexity ----------\n",
    "total_nll = 0.0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask in wiki_loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # For causal LM we can pass labels=input_ids; model returns mean loss over non-ignored tokens\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        # outputs.loss is averaged per token (excluding -100 labels), so multiply by number of tokens to get sum NLL\n",
    "        # Count tokens for this batch:\n",
    "        n_tokens = int(attention_mask.sum().item())\n",
    "        batch_loss = outputs.loss.item() * n_tokens\n",
    "\n",
    "        total_nll += batch_loss\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "per_token_loss = total_nll / total_tokens\n",
    "perplexity = math.exp(per_token_loss)\n",
    "print(f\"WikiText-2 subset tokens: {total_tokens}\")\n",
    "print(f\"Per-token cross-entropy (nats): {per_token_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- SST-2 Zero-shot prompt scoring ----------\n",
    "# Load SST-2\n",
    "sst = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "NUM_SST_EXAMPLES = 512   # keep manageable; change as desired\n",
    "sst = sst.select(range(min(len(sst), NUM_SST_EXAMPLES)))\n",
    "\n",
    "# Simple prompt template and label texts\n",
    "def make_prompt(sentence):\n",
    "    # Use a compact template; you can change wording to test sensitivity.\n",
    "    return f\"Sentence: {sentence}\\nQuestion: Is the sentiment of the sentence positive or negative?\\nAnswer:\"\n",
    "\n",
    "label_texts = [\" positive\", \" negative\"]  # leading space usually helps BPE for GPT-2\n",
    "\n",
    "# Tokenize prompt-only (we will append labels and compute log-probs)\n",
    "def tokenize_prompt(example):\n",
    "    prompt = make_prompt(example[\"sentence\"])\n",
    "    enc = tokenizer(prompt, add_special_tokens=False)\n",
    "    return {\"prompt_ids\": enc[\"input_ids\"], \"label\": example[\"label\"]}\n",
    "\n",
    "sst_tok = sst.map(tokenize_prompt, remove_columns=sst.column_names)\n",
    "\n",
    "# Collate: for each example in a batch we will create two sequences: prompt+label_positive, prompt+label_negative\n",
    "def collate_prompt_label(batch):\n",
    "    # Build a list of sequences (two per example)\n",
    "    seqs = []\n",
    "    seq_meta = []  # (example_idx, label_idx, prompt_len, label_len)\n",
    "    for i, row in enumerate(batch):\n",
    "        prompt_ids = row[\"prompt_ids\"]\n",
    "        for label_idx, label_text in enumerate(label_texts):\n",
    "            label_ids = tokenizer(label_text, add_special_tokens=False)[\"input_ids\"]\n",
    "            seq = prompt_ids + label_ids\n",
    "            seqs.append(torch.tensor(seq, dtype=torch.long))\n",
    "            seq_meta.append((i, label_idx, len(prompt_ids), len(label_ids)))\n",
    "    # pad sequences\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"meta\": seq_meta,\n",
    "        \"orig_labels\": [row[\"label\"] for row in batch],\n",
    "    }\n",
    "\n",
    "BATCH_SIZE_SST = 16\n",
    "sst_loader = DataLoader(sst_tok, batch_size=BATCH_SIZE_SST, collate_fn=collate_prompt_label)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in sst_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        meta = batch[\"meta\"]\n",
    "        orig_labels = batch[\"orig_labels\"]  # length = batch_size\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape (N_seq, seq_len, vocab)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # For each sequence (prompt+label), sum log-probs of label tokens (they are at positions prompt_len .. prompt_len+label_len-1)\n",
    "        seq_scores = []  # one score per sequence (i.e., per prompt+label)\n",
    "        for seq_idx, (example_idx, label_idx, prompt_len, label_len) in enumerate(meta):\n",
    "            # label token positions: absolute indices = prompt_len ... prompt_len+label_len-1\n",
    "            # For each label token at absolute position j, predicted by logits at position j-1\n",
    "            # So we index log_probs[seq_idx, j-1, token_id]\n",
    "            lp_sum = 0.0\n",
    "            for tok_pos in range(label_len):\n",
    "                abs_pos = prompt_len + tok_pos\n",
    "                pred_logits_pos = abs_pos - 1\n",
    "                # If the label starts at position 0 (no prompt), then pred_logits_pos == -1; but our prompts always non-empty.\n",
    "                token_id = input_ids[seq_idx, abs_pos].item()\n",
    "                lp = log_probs[seq_idx, pred_logits_pos, token_id].item()\n",
    "                lp_sum += lp\n",
    "            seq_scores.append(lp_sum)\n",
    "\n",
    "        # seq_scores is length batch_size*2; group every 2 to get label scores per example\n",
    "        for i in range(len(orig_labels)):\n",
    "            score_pos = seq_scores[2 * i + 0]   # since label_texts[0] is \" positive\"\n",
    "            score_neg = seq_scores[2 * i + 1]\n",
    "            pred_label = 1 if score_pos > score_neg else 0  # SST-2 labels: 1 positive, 0 negative\n",
    "            if pred_label == orig_labels[i]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "print(f\"SST-2 zero-shot accuracy on subset ({total} examples): {accuracy:.4%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
