{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7524d1",
   "metadata": {},
   "source": [
    "## Goal of Study\n",
    "\n",
    "### Context\n",
    "- *Multilingual Models (XGLM/mBERT):* These models are not trained on separate English, Spanish, and Hindi datasets. They are trained on a massive, mixed-corpus of many languages at once. The hope is that the model learns abstract, language-agnostic linguistic concepts. For example, the concept of a \"verb\" or \"subject\" should be represented similarly, regardless of whether the word is \"run\"  or \"दौड़ना\" (daudna - Hindi).\n",
    "- *Shared Representation Space:* The tokenizer has a shared vocabulary for all languages. This forces the model to map tokens from different languages that have similar meanings (e.g., \"cat\" and \"gato\") to nearby points in its embedding space. This shared space is what makes transfer possible.\n",
    "\n",
    "### The Zero-Shot Hypothesis\n",
    "If we fine-tune the model on an English task (e.g., sentiment classification), the model learns to move representations of sentences into \"positive\" or \"negative\" regions of its internal space. The hypothesis is that because a Spanish sentence with a positive sentiment is already represented near its English equivalent, it too will be correctly classified, even though the model has never seen a labeled Spanish example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc363e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from src.finetune import train_short_run \n",
    "from src.eval import eval_perplexity, eval_classification_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed6722",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Use XGLM for causal LM (perplexity) tasks\n",
    "# Use mBERT or XLM-R for classification tasks\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\" # Let's use a classifier for a clearer signal\n",
    "TRAIN_DATASET_NAME = \"multi_nli\" # English-only training data\n",
    "EVAL_DATASET_NAME = \"xnli\"\n",
    "EVAL_DATASET_CONFIG = \"all_languages\"\n",
    "LANGS = [\"en\", \"es\", \"fr\", \"hi\"]\n",
    "LANGS = [\"en\", \"es\", \"fr\", \"hi\"]\n",
    "NUM_SAMPLES_PER_LANG = 1000 # Keep it small and fast\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# For classification, we need a different model head\n",
    "# The NLI task has 3 labels: entailment (0), neutral (1), contradiction (2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to(DEVICE)\n",
    "\n",
    "# --- Load and Prepare Datasets ---\n",
    "print(\"Loading English training data from MultiNLI...\")\n",
    "# We only need a small slice for this quick experiment\n",
    "train_dataset = load_dataset(TRAIN_DATASET_NAME, split='train').shuffle(seed=42).select(range(NUM_SAMPLES_PER_LANG * 5)) # Use a bit more for training\n",
    "\n",
    "\n",
    "# Load the multilingual validation data and create separate datasets for each language\n",
    "print(\"Loading and filtering multilingual validation data from XNLI...\")\n",
    "eval_datasets = {}\n",
    "\n",
    "for lang in LANGS:\n",
    "    print(f\"  -> Loading XNLI for language: {lang}\")\n",
    "    # The language code is passed as the second argument to load_dataset\n",
    "    lang_dset = load_dataset(EVAL_DATASET_NAME, lang, split='validation')\n",
    "    \n",
    "    # Select a small sample and store it\n",
    "    eval_datasets[lang] = lang_dset.shuffle(seed=42).select(range(NUM_SAMPLES_PER_LANG))\n",
    "\n",
    "print(f\"Loaded evaluation data for: {list(eval_datasets.keys())}\")\n",
    "# Example: train_dataset is ready, and eval_datasets['es'] is our Spanish test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca8e90",
   "metadata": {},
   "source": [
    "#### Pre-trained Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42035882",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# `eval_classification_accuracy`: It tokenizes batches, feeds them to the model, gets logits, finds the argmax, and compares to labels.\n",
    "\n",
    "baseline_results = {}\n",
    "for lang in LANGS:\n",
    "    print(f\"Evaluating baseline for language: {lang}\")\n",
    "    accuracy = eval_classification_accuracy(model, tokenizer, eval_datasets[lang], DEVICE)\n",
    "    baseline_results[lang] = accuracy\n",
    "    print(f\"  -> Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221750ad",
   "metadata": {},
   "source": [
    "#### Visualizing Cross-Lingual Embeddings (T-SNE)\n",
    "\n",
    "The central idea, demonstrated in papers on mBERT (\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\") and more explicitly in work on sentence embeddings like LASER (\"Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond\"), is that these models learn a language-agnostic semantic space. Sentences with equivalent meanings, even in different languages, are mapped to nearby vectors. To visualize this, we use `T-SNE` by projecting the high-dimensional embeddings into a lower-dimensional space (2-D) while preserving the local structure and relationships between original embeddings.\n",
    "\n",
    "We will extract the sentence representation from the final layer's [CLS] token, which is the standard method for getting a sentence-level embedding from BERT-like models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398663c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Use the base model for this, not the one fine-tuned for classification\n",
    "base_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\", output_hidden_states=True).to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "# 1. Define parallel sentences\n",
    "parallel_sentences = [\n",
    "    (\"The cat is sleeping on the mat.\", \"en\"), # English\n",
    "    (\"El gato está durmiendo en la alfombra.\", \"es\"), # Spanish\n",
    "    (\"Le chat dort sur le tapis.\", \"fr\"), # French\n",
    "    (\"बिल्ली चटाई पर सो रही है।\", \"hi\"), # Hindi\n",
    "    \n",
    "    (\"This restaurant has amazing food.\", \"en\"),\n",
    "    (\"Este restaurante tiene comida increíble.\", \"es\"),\n",
    "    (\"Ce restaurant propose une cuisine incroyable.\", \"fr\"),\n",
    "    (\"इस रेस्टोरेंट का खाना अद्भुत है।\", \"hi\"),\n",
    "\n",
    "    (\"Can you help me with this problem?\", \"en\"),\n",
    "    (\"¿Puedes ayudarme con este problema?\", \"es\"),\n",
    "    (\"Pouvez-vous m'aider avec ce problème ?\", \"fr\"),\n",
    "    (\"क्या आप इस समस्या में मेरी मदद कर सकते हैं?\", \"hi\"),\n",
    "]\n",
    "\n",
    "# 2. Extract [CLS] token embeddings from last layer (for getting a sentence-level embedding from BERT-like models)\n",
    "sentence_embeddings = []\n",
    "sentence_labels = []\n",
    "lang_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sentence, lang in parallel_sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt').to(DEVICE)\n",
    "        outputs = base_model(**inputs)\n",
    "        # Get the last hidden state and take the [CLS] token's representation (at index 0)\n",
    "        cls_embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()\n",
    "        sentence_embeddings.append(cls_embedding)\n",
    "        sentence_labels.append(sentence)\n",
    "        lang_labels.append(lang)\n",
    "\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "# 3. Apply T-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, random_state=42, init='pca', learning_rate='auto')\n",
    "embeddings_2d = tsne.fit_transform(sentence_embeddings)\n",
    "\n",
    "# 4. Plot the results\n",
    "plt.figure(figsize=(14, 10))\n",
    "colors = {'en': 'blue', 'es': 'red', 'fr': 'green', 'hi': 'purple'}\n",
    "lang_names = {'en': 'English', 'es': 'Spanish', 'fr': 'French', 'hi': 'Hindi'}\n",
    "\n",
    "# Plot points\n",
    "for lang in np.unique(lang_labels):\n",
    "    ix = np.where(np.array(lang_labels) == lang)\n",
    "    plt.scatter(embeddings_2d[ix, 0], embeddings_2d[ix, 1], c=colors[lang], label=lang_names[lang], s=100)\n",
    "\n",
    "# Annotate points\n",
    "for i, txt in enumerate(sentence_labels):\n",
    "    # Shorten long sentences for clarity in the plot\n",
    "    display_txt = (txt[:50] + '...') if len(txt) > 50 else txt\n",
    "    plt.annotate(display_txt, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)\n",
    "\n",
    "plt.title(\"T-SNE Visualization of Cross-Lingual Sentence Embeddings\")\n",
    "plt.xlabel(\"T-SNE Dimension 1\")\n",
    "plt.ylabel(\"T-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b119f",
   "metadata": {},
   "source": [
    "#### Interpreting Cross-Lingual Attention Maps\n",
    "\n",
    "Looking at a single attention head can be noisy. As shown in papers like \"Analyzing Multi-Head Self-Attention\" (Voita et al., 2019), different heads specialize. Some act like positional masks, some handle syntactic dependencies, etc. A simple and powerful method for getting a clearer signal, used in many interpretability analyses, is to average the attention probabilities across all heads within a given layer. This smooths out the noise and reveals the dominant, most robust attention pattern at that level of abstraction.\n",
    "\n",
    "We will compare the averaged attention maps for a parallel sentence pair to see if the model is \"reasoning\" about the sentences in a structurally similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab984000",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import seaborn as sns\n",
    "\n",
    "# We need a model that can output attentions, thus we use XGLM for a causal LM example.\n",
    "# NOTE: We can adapt this for mBERT, but we'll get a square attention matrix.\n",
    "# For causal LMs, it's triangular.\n",
    "attn_model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-564M\", output_attentions=True).to(DEVICE)\n",
    "attn_tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-564M\")\n",
    "attn_model.eval()\n",
    "\n",
    "def plot_cross_lingual_attention(sent1, sent2, layer_idx, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Plots the average attention heatmaps for two sentences side-by-side.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    for i, (ax, sent) in enumerate(zip(axes, [sent1, sent2])):\n",
    "        inputs = tokenizer(sent, return_tensors='pt').to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # attentions is a tuple of (num_layers) tensors\n",
    "        # Each tensor is (batch_size, num_heads, seq_len, seq_len)\n",
    "        attentions = outputs.attentions[layer_idx]\n",
    "        \n",
    "        # Average across all heads\n",
    "        avg_attention = attentions.squeeze(0).mean(dim=0).cpu().numpy()\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        sns.heatmap(avg_attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax)\n",
    "        ax.set_title(f\"Average Attention (Layer {layer_idx})\\nSentence {i+1}: '{sent}'\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: \"The black cat\" vs \"El gato negro\"\n",
    "# We want to see if \"cat\" attends to \"black\" similarly to how \"gato\" attends to \"negro\"\n",
    "eng_sent = \"The black cat sat down.\"\n",
    "esp_sent = \"El gato negro se sentó.\"\n",
    "\n",
    "# A mid-level layer is often good for semantic/syntactic relationships\n",
    "plot_cross_lingual_attention(eng_sent, esp_sent, layer_idx=12, model=attn_model, tokenizer=attn_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772929cd",
   "metadata": {},
   "source": [
    "#### Finetune on english only dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da5506",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.train import train_classifier\n",
    "\n",
    "print(\"Fine-tuning on English...\")\n",
    "# This function should return the fine-tuned model\n",
    "tuned_model = train_classifier(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    device=DEVICE,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc503ed",
   "metadata": {},
   "source": [
    "#### Zero-shot evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbb4b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "zero_shot_results = {}\n",
    "for lang in LANGS:\n",
    "    print(f\"Evaluating zero-shot for language: {lang}\")\n",
    "    accuracy = eval_classification_accuracy(tuned_model, tokenizer, datasets[lang], DEVICE)\n",
    "    zero_shot_results[lang] = accuracy\n",
    "    print(f\"  -> Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc038f51",
   "metadata": {},
   "source": [
    "#### Questions to Study:\n",
    "- Baseline Performance: Is the pretrained mBERT model better than random (20% for 5 stars)? It should be, showing it already has some cross-lingual understanding.\n",
    "- Performance on English: How much did fine-tuning improve the English accuracy? This is your sanity check. zero_shot_results['en'] should be much higher than baseline_results['en'].\n",
    "- The Transfer Gap: Compare zero_shot_results for es, fr, hi to the tuned English performance. The difference is the \"transfer gap.\"\n",
    "- Linguistic Distance: This is the key insight. You will likely see that performance drops as the language gets more distant from English.\n",
    "    - es, fr (Romance languages): Should transfer very well. They share Latin roots and sentence structure with English.\n",
    "    - hi (Indo-Aryan language): Will likely see the biggest performance drop. It uses a different script (Devanagari) and has different grammatical structures. The shared vocabulary is much smaller."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
