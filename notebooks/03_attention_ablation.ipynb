{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c1f6d3",
   "metadata": {},
   "source": [
    "# Goal of the study\n",
    "\n",
    "This notebook explores two aspects of transformer model training: optimizer performance and the role of the self-attention mechanism. We will:\n",
    "1. Compare the `AdamW` and `SGD` optimizers on a small fine-tuning task.\n",
    "2. Implement and evaluate an \"attention ablation\" to understand the impact of learned attention on model performance. This we do using `hooks` in pytorch\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and functions from our `src/finetune.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77408fcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from src.finetune import (\n",
    "    build_tiny_dataset,\n",
    "    collate_pad,\n",
    "    train_short_run,\n",
    "    eval_perplexity,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c8684",
   "metadata": {},
   "source": [
    "## Attention Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04b402",
   "metadata": {},
   "source": [
    "### Hooks and their types\n",
    "\n",
    "| Hook Function                 | Target        | Execution Time                 | Signature                   | Can Modify                                   |\n",
    "| ----------------------------- | ------------- | ------------------------------ | --------------------------- | -------------------------------------------- |\n",
    "| `register_forward_pre_hook`   | `nn.Module`   | Before `forward()`             | `(module, input)`           | Module's input                               |\n",
    "| `register_forward_hook`       | `nn.Module`   | After `forward()`              | `(module, input, output)`   | Module's output                              |\n",
    "| `register_full_backward_hook` | `nn.Module`   | During backward pass           | `(module, grad_in, grad_out)` | Gradient w.r.t. module's input (`grad_in`)   |\n",
    "| `register_hook`               | `torch.Tensor`| When grad for that tensor is computed | `(grad)`                    | The tensor's gradient (`grad`)                |\n",
    "\n",
    "### Usefulness of Hooks\n",
    "1. Inspecting code\n",
    "2. Logging internals of a model\n",
    "3.  Modify internal state (like activations and gradients) without change source code of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e39c37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from src.attention_ablation import (\n",
    "    attention_ablation_hook,\n",
    "    apply_attention_ablation,\n",
    "    train_ablation_run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229e5ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tiny_dataset, _ = build_tiny_dataset(tokenizer, split=\"train\")\n",
    "val_dataset, _ = build_tiny_dataset(tokenizer, split=\"validation\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=lambda b: collate_pad(b, pad_id=tokenizer.pad_token_id))\n",
    "\n",
    "# --- Optimizer Comparison ---\n",
    "print(\"Running optimizer comparison...\")\n",
    "adamw_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"adamw_notebook\",\n",
    "    epochs=5,\n",
    "    optimizer_name=\"AdamW\",\n",
    ")\n",
    "\n",
    "sgd_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"sgd_notebook\",\n",
    "    epochs=5,\n",
    "    optimizer_name=\"SGD\",\n",
    ")\n",
    "\n",
    "# Plotting optimizer results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(adamw_res[\"loss_values\"], label=\"AdamW\")\n",
    "plt.plot(sgd_res[\"loss_values\"], label=\"SGD\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Optimizer Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4966dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Attention Ablation ---\n",
    "print(\"\\nRunning attention ablation...\")\n",
    "ablation_res = train_ablation_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    ")\n",
    "\n",
    "# Compare with a normal run\n",
    "print(\"\\nRunning baseline for ablation comparison...\")\n",
    "baseline_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"baseline_notebook\",\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "\n",
    "# Plotting ablation results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(baseline_res[\"perplexities\"], label=\"Baseline (with attention)\")\n",
    "plt.plot(ablation_res[\"perplexities\"], label=\"Attention Ablated\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.title(\"Attention Ablation Study\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d088f1",
   "metadata": {},
   "source": [
    "## Analysis and Conclusion\n",
    "\n",
    "**Optimizer Comparison:**\n",
    "- *Observations:* In our short run, AdamW converges faster and more stably than SGD. SGD might show more fluctuations in the loss.\n",
    "- *Conclusion:* AdamW is generally a better choice for training large language models due to its adaptive learning rate.\n",
    "\n",
    "**Attention Ablation:**\n",
    "- *Observations:* The model with ablated attention has a significantly higher perplexity. This is because it cannot learn context-dependent relationships between tokens. However, even without self-attention, the token representation do get better over time\n",
    "- *Conclusion:* This experiment demonstrates the critical role of the self-attention mechanism in language modeling. Without it, the model's performance degrades substantially."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
