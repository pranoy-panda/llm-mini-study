{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c1f6d3",
   "metadata": {},
   "source": [
    "# Attention Ablation\n",
    "\n",
    "This notebook explores two aspects of transformer model training: optimizer performance and the role of the self-attention mechanism. We will:\n",
    "1. Compare the `AdamW` and `SGD` optimizers on a small fine-tuning task.\n",
    "2. Implement and evaluate an \"attention ablation\" to understand the impact of learned attention on model performance.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and functions from our `finetune.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77408fcd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Assuming finetune.py is in the same directory or in the python path\n",
    "from finetune import (\n",
    "    build_tiny_dataset,\n",
    "    collate_pad,\n",
    "    train_short_run,\n",
    "    eval_perplexity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e39c37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Attention Ablation Code ---\n",
    "def attention_ablation_hook(module, input, output):\n",
    "    attention_scores = output\n",
    "    ablated_scores = torch.zeros_like(attention_scores)\n",
    "    return (ablated_scores,) + output[1:]\n",
    "\n",
    "def apply_attention_ablation(model):\n",
    "    for layer in model.transformer.h:\n",
    "        # The actual attribute name may vary depending on the model architecture.\n",
    "        # For GPT-2, it's layer.attn.\n",
    "        # You may need to inspect the model to find the correct attribute.\n",
    "        layer.attn.register_forward_pre_hook(attention_ablation_hook)\n",
    "\n",
    "def train_ablation_run(\n",
    "    base_model_name: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: str,\n",
    "    tiny_dataset,\n",
    "    val_loader,\n",
    "    run_name: str = \"ablation\",\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 4,\n",
    "    lr: float = 1e-4,\n",
    "    save_dir: str = \"outputs\",\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model_name).to(device)\n",
    "    apply_attention_ablation(model)\n",
    "    model.train()\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    loader = DataLoader(tiny_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda b: collate_pad(b, pad_id=tokenizer.pad_token_id))\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    perplexities = []\n",
    "    for epoch in range(epochs):\n",
    "        for input_ids, attention_mask in loader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        epoch_perplexity = eval_perplexity(model, val_loader, device, tokenizer.pad_token_id)\n",
    "        perplexities.append(epoch_perplexity)\n",
    "        print(f\"{run_name} epoch {epoch}: val_perplexity={epoch_perplexity:.4f}\")\n",
    "    \n",
    "    return {\"loss_values\": losses, \"perplexities\": perplexities}\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tiny_dataset, _ = build_tiny_dataset(tokenizer, split=\"train\")\n",
    "val_dataset, _ = build_tiny_dataset(tokenizer, split=\"validation\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=lambda b: collate_pad(b, pad_id=tokenizer.pad_token_id))\n",
    "\n",
    "# --- Optimizer Comparison ---\n",
    "print(\"Running optimizer comparison...\")\n",
    "adamw_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"adamw_notebook\",\n",
    "    epochs=5,\n",
    "    optimizer_name=\"AdamW\",\n",
    ")\n",
    "\n",
    "sgd_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"sgd_notebook\",\n",
    "    epochs=5,\n",
    "    optimizer_name=\"SGD\",\n",
    ")\n",
    "\n",
    "# Plotting optimizer results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(adamw_res[\"loss_values\"], label=\"AdamW\")\n",
    "plt.plot(sgd_res[\"loss_values\"], label=\"SGD\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Optimizer Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Attention Ablation ---\n",
    "print(\"\\nRunning attention ablation...\")\n",
    "ablation_res = train_ablation_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    ")\n",
    "\n",
    "# Compare with a normal run\n",
    "print(\"\\nRunning baseline for ablation comparison...\")\n",
    "baseline_res = train_short_run(\n",
    "    base_model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    tiny_dataset=tiny_dataset,\n",
    "    val_loader=val_loader,\n",
    "    run_name=\"baseline_notebook\",\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "\n",
    "# Plotting ablation results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(baseline_res[\"perplexities\"], label=\"Baseline (with attention)\")\n",
    "plt.plot(ablation_res[\"perplexities\"], label=\"Attention Ablated\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.title(\"Attention Ablation Study\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d088f1",
   "metadata": {},
   "source": [
    "## Analysis and Conclusion\n",
    "\n",
    "**Optimizer Comparison:**\n",
    "- *Observations:* In our short run, AdamW is expected to converge faster and more stably than SGD. SGD might show more fluctuations in the loss.\n",
    "- *Conclusion:* AdamW is generally a better choice for training large language models due to its adaptive learning rate.\n",
    "\n",
    "**Attention Ablation:**\n",
    "- *Observations:* The model with ablated attention should have a significantly higher perplexity. This is because it cannot learn context-dependent relationships between tokens.\n",
    "- *Conclusion:* This experiment demonstrates the critical role of the self-attention mechanism in language modeling. Without it, the model's performance degrades substantially."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
